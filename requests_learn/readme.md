## requests modules

* 首先我们书写代码的时候，我们首先需要的就是先确定我们的发送网络请求的网址 `url`
  * `url` 这个就是我们的统一资源定位符`uniform resourse location`
  * 然后使用模块对我们的网址发送请求即可，然后就会然后一个响应的内容返回
    * 需要我们注意的是： 我们发送的请求可以是：
      * `get` 请求
      * `post` 请求
      * `put` 请求


* 实现了获取响应内容后，可以对响应内容做出的处理:
  * `.text` 获取简单的html网页
  * `.status_code` 获取响应状态码


* 发送携带参数的 `get` 请求
  * 一般的话，我们就是直接使用的我们的创建一个空字典设置携带的参数
  * 然后使用 `params` 参数实现接收数据即可


* 发送携带参数的 `post` 请求
  * 还是使用的是我们的创建一个空字典，将需要传输的数据使用 `data` 实现接收需要传入的数据即可
  * 如果说吧是我们的携带参数的请求，那么还是通过 `params` 接收即可


* 获取 json 数据
  * 直接将我们的返回数据通过 `.json()` 实现转换
  * 就可以实现我们的获取得到 `json` 数据


* 获取二进制数据
  * `.content` 就可以实现获取二进制数据


* 实现初步伪装爬虫
  * 这个的实现的最基本的步骤就是实现添加我们的 `headers` 来实现伪装自己
  * 为什么需要伪装：
    * 因为在实现向一个网页发送请求的时候，单纯的请求可能会导致被认为是恶意攻击
    * 这个时候，我们就可以通过配置 `headers` 信息来实现伪装自己，不被认为是恶意攻击
    * 这个配置信息的由来是通过的 `cv大法` 就可以实现获取了
    * 当你被认为是一个爬虫用户的时候，就有可能导致 `封ip`
    * 一般的话，只用复制： `user-agent` 即可
  * 然后请求的时候，把 `headers` 带上就行了


## 重要知识

* 会话维持
  * 首先我们的额  `http/https` 本身就是一个 `无状态的协议` ，对事物是不具备记忆功能的，每一次的运行都是一次独立的状态
  * 为了解决这个问题，就出现了 `cookie` 和 `session` 来实现解决这个 `无状态协议` 问题
  * 这个理解就是：
    * 如果你和我一样的，喜欢用的是 `pc端` 的 `b站` ，当没有这个会话维持的时候
    * 那么最后导致的就是，你每次进去后都需要进行一次登录
    * 或者说每次刷新网页的时候，也是需要进行一次登录提醒
  * `cookie` 一般来说，这个就是登录用户的一种用户标识，然后通过浏览器缓存实现会话维持 (是有过期时间的)
    * 这个还是添加到 `headers` 配置信息里面的
  * `session`
    * 就是实现使用我们的session 来实现发送我们的请求
      * 这个时候，`服务端` 就可以知道一个用户发送的请求
```python
import requets
# 步骤一
sess = requests.session()

sess.get(url, 配置信息)
```


* 配置代理信息
  * 有的时候，我们实现请求的时候，肯定是不使用自己本机去发送请求的
  * 这个时候就需要配置代理信息
  * 代理信息: `http` `https` 同时实现设置
  * 配置信息的字段就是: `proxies` ，这个的配置信息里面携带了我们的 `ip地址` 信息


* 请求超时设置
  * 就是在请求的时候设置字段: `timeout` 后面实现设置的时间就是我们的 `秒(只是单位)`    